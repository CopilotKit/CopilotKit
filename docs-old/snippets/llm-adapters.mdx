import { Callout } from 'nextra/components'

LLM Adapters are responsible for executing the request with the LLM and standardizing the request/response format in a way that the Copilot Runtime can understand.

Currently, we support the following LLM adapters natively:

- [OpenAIAdapter](/reference/classes/CopilotRuntime/llm-adapters/OpenAIAdapter)
- [OpenAIAssistantAdapter](/reference/classes/CopilotRuntime/llm-adapters/OpenAIAssistantAdapter)
- [LangChainAdapter](/reference/classes/CopilotRuntime/llm-adapters/LangChainAdapter)
- [GroqAdapter](/reference/classes/CopilotRuntime/llm-adapters/GroqAdapter)
- [GoogleGenerativeAIAdapter](/reference/classes/CopilotRuntime/llm-adapters/GoogleGenerativeAIAdapter)
- [AnthropicAdapter](/reference/classes/CopilotRuntime/llm-adapters/AnthropicAdapter)


<Callout>
  You can use the **`LangChainAdapter`** to use any LLM provider we don't yet natively support!
</Callout>

<Callout>
  It's not too hard to write your own LLM adapter from scratch -- see the existing adapters for inspiration. And of course, we would love a contribution! ⭐️
</Callout>