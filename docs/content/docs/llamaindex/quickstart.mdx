---
title: Quickstart
description: Turn your LlamaIndex Agents into an agent-native application in 10 minutes.
icon: "lucide/Play"
hideTOC: true
---

import {
  TailoredContent,
  TailoredContentOption,
} from "@/components/react/tailored-content.tsx";
import { CoAgentsEnterpriseCTA } from "@/components/react/coagents/coagents-enterprise-cta.tsx";
import { CoAgentsDiagram } from "@/components/react/coagents/coagents-diagram.tsx";
import { FaPython, FaJs, FaCloud } from "react-icons/fa";
import SelfHostingCopilotRuntimeCreateEndpoint from "@/snippets/self-hosting-copilot-runtime-create-endpoint.mdx";
import CopilotKitCloudCopilotKitProvider from "@/snippets/copilot-cloud-configure-copilotkit-provider.mdx";
import { Accordions, Accordion } from "fumadocs-ui/components/accordion";
import FindYourCopilotRuntime from "@/snippets/find-your-copilot-runtime.mdx";
import CloudCopilotKitProvider from "@/snippets/coagents/cloud-configure-copilotkit-provider.mdx";
import SelfHostingCopilotRuntimeConfigureCopilotKitProvider from "@/snippets/coagents/self-host-configure-copilotkit-provider.mdx";
import SelfHostingCopilotRuntimeStarter from "@/snippets/self-hosting-copilot-runtime-starter.mdx";
import SelfHostingRemoteEndpoints from "@/snippets/self-hosting-remote-endpoints.mdx";
import {
  UserIcon,
  PaintbrushIcon,
  WrenchIcon,
  RepeatIcon,
  ServerIcon,
} from "lucide-react";
import CopilotUI from "@/snippets/copilot-ui.mdx";

<video
  src="https://cdn.copilotkit.ai/docs/copilotkit/images/coagents/chat-example.mp4"
  className="rounded-lg shadow-xl"
  loop
  playsInline
  controls
  autoPlay
  muted
/>

## Prerequisites

Before you begin, you'll need the following:

- An OpenAI API key
- Node.js 20+
- Python 3.10+
- Your favorite Node package manager
- Your favorite Python package manager

## Getting started

<Steps>
    <TailoredContent
        className="step"
        id="agent"
        header={
            <div>
                <p className="text-xl font-semibold">Choose your starting point</p>
                <p className="text-base">
                    You can either start fresh with our starter template or integrate CopilotKit into your existing LlamaIndex Agent.
                </p>
            </div>
        }
    >
        <TailoredContentOption
            id="cli"
            title="Start from scratch"
            description="Get started quickly with our ready-to-go starter application."
        >
            <Step>
                ### Run our CLI
                First, we'll use our CLI to create a new project for us.

                ```bash
                npx copilotkit@latest create -f llamaindex
                ```
            </Step>
            <Step>
                ### Install dependencies

                Now we need to install the depedencies for the frontend and the agent.

                <Callout type="info">
                  The starter comes with a post-install script to install both the frontend and agent dependencies. 
                </Callout>

                <Tabs groupId="package-manager" items={['npm', 'pnpm', 'yarn', 'bun']}>
                    <Tab value="npm">
                        ```bash
                        npm install
                        ```
                    </Tab>
                    <Tab value="pnpm">
                        ```bash
                        pnpm install
                        ```
                    </Tab>
                    <Tab value="yarn">
                        ```bash
                        yarn install
                        ```
                    </Tab>
                    <Tab value="bun">
                        ```bash
                        bun install
                        ```
                    </Tab>
                </Tabs>
            </Step>
            <Step>
                ### Configure your environment

                This LlamaIndex agent uses OpenAI's `gpt-4o` by default, so we need to set the `OPENAI_API_KEY` environment variable.

                ```bash
                export OPENAI_API_KEY="your_openai_api_key"
                ```

                <Callout type="info" title="What about other models?">
                  The starter template is configured to use OpenAI's GPT-4o by default, but you can modify it to use any language model supported by LlamaIndex.
                </Callout>
            </Step>
            <Step>
                ### Start the development server
                This will start both the frontend and agent servers concurrently.

                <Tabs groupId="package-manager" items={['npm', 'pnpm', 'yarn', 'bun']}>
                    <Tab value="npm">
                        ```bash
                        npm run dev
                        ```
                    </Tab>
                    <Tab value="pnpm">
                        ```bash
                        pnpm dev
                        ```
                    </Tab>
                    <Tab value="yarn">
                        ```bash
                        yarn dev
                        ```
                    </Tab>
                    <Tab value="bun">
                        ```bash
                        bun dev
                        ```
                    </Tab>
                </Tabs>

                This will start both the UI and agent servers concurrently.
            </Step>
        </TailoredContentOption>
        <TailoredContentOption
            id="bring-your-own"
            title="Code along"
            description="I want to start from scratch and code along with the example."
        >
            <Step>
                ### Add necessary dependencies

                First, we'll need to make sure we have the necessary dependencies installed.
                
                <Tabs groupId="python-pm" items={['pip', 'poetry', 'uv']}>
                    <Tab value="pip">
                        ```bash
                        pip install llama-index llama-index-llms-openai llama-index-protocols-ag-ui fastapi uvicorn
                        ```
                    </Tab>
                    <Tab value="poetry">
                        ```bash
                        poetry init
                        poetry add llama-index llama-index-llms-openai llama-index-protocols-ag-ui fastapi uvicorn
                        ```
                    </Tab>
                    <Tab value="uv">
                        ```bash
                        uv init
                        uv add llama-index llama-index-llms-openai llama-index-protocols-ag-ui fastapi uvicorn
                        ```
                    </Tab>
                </Tabs>
            </Step>
            <Step>
                ### Frontend Setup
                CopilotKit works with any React-based frontend. We'll use Next.js for this example.

                ```bash
                npx create-next-app@latest my-copilot-app
                cd my-copilot-app
                ```
            </Step>
            <Step>
                ### Install CopilotKit packages

                ```package-install
                @copilotkit/react-ui @copilotkit/react-core @copilotkit/runtime @ag-ui/llamaindex
                ```
            </Step>
            <Step>
                ### Create your LlamaIndex agent server

                Create a new file called `agent.py` in your project root:

                ```python title="agent.py"
                import os
                from fastapi import FastAPI
                from llama_index.llms.openai import OpenAI
                from llama_index.protocols.ag_ui.router import get_ag_ui_workflow_router

                # Initialize the LLM
                llm = OpenAI(
                    model="gpt-4o",  # Use gpt-4o for better performance
                    api_key=os.getenv("OPENAI_API_KEY")
                )

                # Create the AG-UI workflow router
                agentic_chat_router = get_ag_ui_workflow_router(
                    llm=llm,
                    system_prompt="You are a helpful AI assistant with access to various tools and capabilities.",
                )

                # Create FastAPI app
                app = FastAPI(
                    title="LlamaIndex Agent",
                    description="A LlamaIndex agent integrated with CopilotKit",
                    version="1.0.0"
                )

                # Include the router
                app.include_router(agentic_chat_router)

                # Health check endpoint
                @app.get("/health")
                async def health_check():
                    return {"status": "healthy", "agent": "llamaindex"}

                if __name__ == "__main__":
                    import uvicorn
                    uvicorn.run(app, host="0.0.0.0", port=8000)
                ```

                <Callout type="info" title="AG-UI Integration">
                  The `get_ag_ui_workflow_router` creates a standardized API endpoint that CopilotKit can communicate with. This allows your LlamaIndex agent to work seamlessly with the CopilotKit frontend.
                </Callout>
            </Step>
            <Step>
                ### Setup Copilot Runtime

                CopilotKit requires a Copilot Runtime endpoint to safely communicate with your agent. This acts as a bridge between your frontend and the LlamaIndex agent.

                Create a new file at `app/api/copilotkit/route.ts` and copy/paste this snippet into it.

                ```ts title="app/api/copilotkit/route.ts"
                import {
                  CopilotRuntime,
                  ExperimentalEmptyAdapter,
                  copilotRuntimeNextJSAppRouterEndpoint,
                } from "@copilotkit/runtime";
                import { LlamaIndexAgent } from "@ag-ui/llamaindex";
                import { NextRequest } from "next/server";

                // Service adapter for multi-agent support (using empty adapter for single agent)
                const serviceAdapter = new ExperimentalEmptyAdapter();

                // Create the CopilotRuntime with LlamaIndex agent integration
                const runtime = new CopilotRuntime({
                  agents: {
                    // Connect to your LlamaIndex agent running on port 8000
                    "llamaindex_agent": new LlamaIndexAgent({
                      url: "http://localhost:8000/run"
                    }),
                  }   
                });

                // Handle CopilotKit runtime requests
                export const POST = async (req: NextRequest) => {
                  const { handleRequest } = copilotRuntimeNextJSAppRouterEndpoint({
                    runtime,
                    serviceAdapter,
                    endpoint: "/api/copilotkit",
                  });
                
                  return handleRequest(req);
                };
                ```

                <Callout type="info" title="Runtime Configuration">
                  The CopilotRuntime acts as a secure proxy between your frontend and the LlamaIndex agent. It handles authentication, request routing, and response formatting.
                </Callout>
            </Step>
            <Step>
                ### Configure CopilotKit Provider

                Wrap your application with the CopilotKit provider to enable AI functionality throughout your app.

                You can just copy/paste this snippet into your existing `layout.tsx` file.

                ```tsx title="app/layout.tsx"
                import { CopilotKit } from "@copilotkit/react-core";
                import "@/app/globals.css";
                import "@copilotkit/react-ui/styles.css";

                export default function RootLayout({ 
                  children 
                }: { 
                  children: React.ReactNode 
                }) {
                  return (
                    <html lang="en">
                      <body>
                        <CopilotKit 
                          runtimeUrl="/api/copilotkit" 
                          agent="llamaindex_agent"
                        >
                          {children}
                        </CopilotKit>
                      </body>
                    </html>
                  );
                }
                ```

                <Callout type="info" title="Provider Configuration">
                  The `CopilotKit` provider makes AI functionality available throughout your React component tree. The `agentName` should match the agent key you defined in the runtime configuration.
                </Callout>
            </Step>
            <Step>
              ### Add chat to your application

              Add a CopilotChat component to your page to enable users to interact with your LlamaIndex agent.

              You can just copy/paste this snippet into your existing `page.tsx` file.

              ```tsx title="app/page.tsx"
              import { CopilotChat } from "@copilotkit/react-ui";

              export default function Page() {
                return (
                  <main className="min-h-screen p-8 bg-indigo-500/20">
                    <CopilotChat 
                      labels={{
                        initial: ["Howdy! I'm connected to your LlamaIndex agent ðŸ¦™"],
                      }}
                      className="w-[600px] h-[80vh] mx-auto border border-indigo-300 rounded-xl p-4"
                    />
                  </main>
                );
              }
              ```

              <Callout type="info" title="UI Components">
                Feel free to also use `CopilotSidebar` and `CopilotPopup` for modal-based chat interfaces.
              </Callout>
            </Step>
            <Step>
              ### Start both servers

              You'll need to run both the agent server and frontend server. Open two terminal windows:

              **Terminal 1**: Start the LlamaIndex agent:

              <Tabs groupId="python-pm" items={['pip', 'poetry', 'uv']}>
                <Tab value="pip">
                  ```bash
                  export OPENAI_API_KEY="your_openai_api_key"
                  python agent.py
                  ```
                </Tab>
                <Tab value="poetry">
                  ```bash
                  export OPENAI_API_KEY="your_openai_api_key"
                  poetry run python agent.py
                  ```
                </Tab>
                <Tab value="uv">
                  ```bash
                  export OPENAI_API_KEY="your_openai_api_key"
                  uv run python agent.py
                  ```
                </Tab>
              </Tabs>

              **Terminal 2**: Start the frontend server:

              <Tabs groupId="package-manager" items={['npm', 'pnpm', 'yarn', 'bun']}>
                <Tab value="npm">
                  ```bash
                  npm run dev
                  ```
                </Tab>
                <Tab value="pnpm">
                  ```bash
                  pnpm dev
                  ```
                </Tab>
                <Tab value="yarn">
                  ```bash
                  yarn dev
                  ```
                </Tab>
                <Tab value="bun">
                  ```bash
                  bun dev
                  ```
                </Tab>
              </Tabs>

              <Callout type="info" title="Server URLs">
                - Agent server: http://localhost:8000
                - Frontend server: http://localhost:3000
                - Agent health check: http://localhost:8000/health
              </Callout>
            </Step>
        </TailoredContentOption>
    </TailoredContent>
    <Step>
        ### ðŸŽ‰ Start chatting!

        Your agent is now ready to use! 
        
        Navigate to http://localhost:3000 and start chatting with it:

        ```
        What tools do you have access to?
        ```

        ```
        What do you think about React?
        ```

        ```
        Show me some cool things you can do!
        ```

        <Accordions className="mb-4">
            <Accordion title="Troubleshooting">
                <div className="space-y-2">
                    <p><strong>Connection Issues:</strong></p>
                    <ul className="list-disc list-inside ml-4 space-y-1">
                        <li>Try using <code>0.0.0.0</code> or <code>127.0.0.1</code> instead of <code>localhost</code></li>
                        <li>Ensure your LlamaIndex agent is running on port 8000</li>
                        <li>Check that the agent health endpoint responds: <code>curl http://localhost:8000/health</code></li>
                    </ul>
                    
                    <p><strong>API Key Issues:</strong></p>
                    <ul className="list-disc list-inside ml-4 space-y-1">
                        <li>Verify your OpenAI API key is correctly set: <code>echo $OPENAI_API_KEY</code></li>
                        <li>Check that your API key has sufficient credits</li>
                        <li>Ensure the key has access to the GPT-4o model</li>
                    </ul>
                    
                    <p><strong>Frontend Issues:</strong></p>
                    <ul className="list-disc list-inside ml-4 space-y-1">
                        <li>Check browser console for errors</li>
                        <li>Verify the CopilotKit provider is properly configured</li>
                        <li>Ensure the runtime URL matches your API route</li>
                    </ul>
                </div>
            </Accordion>
        </Accordions>
    </Step>
</Steps>

## What's next?

Now that you have your basic agent setup, explore these advanced features:

<Cards>
  <Card
    title="Implement Human in the Loop"
    description="Allow your users and agents to collaborate together on tasks."
    href="/llamaindex/human-in-the-loop"
    icon={<UserIcon />}
  />
  <Card
    title="Add some generative UI"
    description="Render your agent's progress and output in the UI."
    href="/llamaindex/generative-ui"
    icon={<PaintbrushIcon />}
  />
  <Card
    title="Setup frontend actions"
    description="Give your agent the ability to call frontend tools, directly updating your application."
    href="/llamaindex/frontend-actions"
    icon={<WrenchIcon />}
  />
</Cards> 