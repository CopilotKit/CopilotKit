---
title: "Stream intermediate state"
---
import { CoAgentsEnterpriseCTA } from "@/components/react/coagents/coagents-enterprise-cta.tsx";


## Background

A LangGraph agentâ€™s state updates discontinuosly; only across node transitions in the graph.

But even a _single node_ in the graph often takes many seconds to run and contain sub-steps of interest to the user -- as nodes engage with LLMs and network resources.

**Agent-native applications** reflect to the end-user what the agent is doing **<u>as continuously possible.</u>**

<Callout>
Continuously showing the end-user the agent's actions is the **gold standard for agentic UX**.

It not only keeps the user engaged (avoiding long loading indicators); it also serves to **build trust** with the user that the agent is on the right track (and that its results can be trusted).

Continuous state updates also play a crucial role in [**_agent steering_**](/coagents/advanced/agent-steering) -- allowing the end-user to correct the agent's course if it veers off.
</Callout>

---

## Stream intermediate state from LLM tool calls:

A LangGraph node's end-state is often driven by an LLM's response.

With `emit_intermediate_state`, you can have CopilotKit treat a given tool call's argument as intermediate state.
The frontend will see the state updates as they stream in.

In this example, we're asking for the `outline` state-key to be streamed as it comes in from the `set_outline` tool.

```python
from copilotkit.langchain import copilotkit_customize_config

async def streaming_state_node(state: AgentState, config: RunnableConfig):
	modifiedConfig = copilotkit_customize_config( # [!code highlight:15]
	    config,

	    # this will stream the `set_outline` tool-call's `outline` argument, *as if* it was the `outline` state parameter.
		# i.e.:
	    # the tool call: set_outline(outline: string)
	    # the state: { ..., outline: "..." }
	    emit_intermediate_state=[
		    {
			    "state_key": "outline", # the name of the key on the agent state we want to interact with
			    "tool": "set_outline", # the name of the tool call
			    "tool_argument": "outline", # the name of the argument on the tool call to treat as intermediate state.
		    }
	    ]
	)

	# Pass the modified configuration to LangChain as an argument
	response = await ChatOpenAI(model="gpt-4o").ainvoke(
		[
			*state["messages"]
		],
		modifiedConfig # pass the modified config # [!code highlight]
	)

	return {
		*state,
		"outline": outline # <- the actual state object is always the final source of truth when the node ends # [!code highlight]
	}
```

## Stream intermediate state manually:
TODO.

<CoAgentsEnterpriseCTA />
