---
title: "Stream intermediate state"
---
import { CoAgentsEnterpriseCTA } from "@/components/react/coagents/coagents-enterprise-cta.tsx";


## Background

A LangGraph agent’s state updates discontinuosly; only across node transitions in the graph.
But even a _single node_ in the graph often takes many seconds to run and contain sub-steps of interest to the user.

**Agent-native applications** reflect to the end-user what the agent is doing **<u>as continuously possible.</u>**

A few key benefits:<br/>
• **<u>Keeps users engaged</u>** by avoiding long loading indicators<br/>
• **<u>Builds trust</u>** by demonstrating what the agent is working on<br/>
• Supports [**_agent steering_**](/coagents/advanced/agent-steering) - allowing users to course-correct the agent if needed<br/>

---


## Stream intermediate state manually:
```python
from copilotkit.langchain import copilotkit_emit_state

async def some_langgraph_node(state: AgentState, config: RunnableConfig):
	# ... whatever you want to docs

	# modify the state as you please: # [!code highlight:5]
	state["logs"].append({
		"message": f"Downloading {resource['url']}",
        "done": False
	})

    await copilotkit_emit_state(config, state) # manually emit the updated state # [!code highlight:1]

	# ... keep running your graph

	# finally, return the state from the node as usual. The returned state is the ultimate source of truth # [!code highlight:2]
	return state
```


## Stream intermediate state from LLM tool calls:

A LangGraph node's end-state is often driven by an LLM's response.

With `emit_intermediate_state`, you can have CopilotKit treat a given tool call's argument as intermediate state.
The frontend will see the state updates as they stream in.

In this example, we're asking for the `outline` state-key to be streamed as it comes in from the `set_outline` tool.

```python
from copilotkit.langchain import copilotkit_customize_config

async def streaming_state_node(state: AgentState, config: RunnableConfig):
	modifiedConfig = copilotkit_customize_config( # [!code highlight:15]
	    config,

	    # this will stream the `set_outline` tool-call's `outline` argument, *as if* it was the `outline` state parameter.
		# i.e.:
	    # the tool call: set_outline(outline: string)
	    # the state: { ..., outline: "..." }
	    emit_intermediate_state=[
		    {
			    "state_key": "outline", # the name of the key on the agent state we want to interact with
			    "tool": "set_outline", # the name of the tool call
			    "tool_argument": "outline", # the name of the argument on the tool call to treat as intermediate state.
		    }
	    ]
	)

	# Pass the modified configuration to LangChain as an argument
	response = await ChatOpenAI(model="gpt-4o").ainvoke(
		[
			*state["messages"]
		],
		modifiedConfig # pass the modified config # [!code highlight]
	)

	# Make sure to ALSO return the state from the node.
	return {
		*state,
		"outline": outline # <- the actual state object is always the final source of truth when the node ends # [!code highlight]
	}
```


<CoAgentsEnterpriseCTA />
