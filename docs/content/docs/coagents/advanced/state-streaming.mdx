---
title: "Managing state streaming"
---
import { CoAgentsEnterpriseCTA } from "@/components/react/coagents/coagents-enterprise-cta.tsx";
import InstallSDKSnippet from "@/snippets/install-sdk.mdx"

By integrating with LangGraph agents on both the backend and frontend, CopilotKit is able to keep your Agentic Copilot's state in sync with your UI in real time, making it as easy to work with as local state while providing immediate updates as the agent works on requests.

To illustrate, imagine a LangGraph agent starts with an initial state of `{"status": "pending"}`, which would propagate to your CopilotKit frontend as the agent performs tasks along its workflow graph.

At some point, a workflow node changes the state to `{"status": "done"}` after performing its task:

<Tabs items={['Python', 'TypeScript']} default="Python">
    <Tab value="Python">
        ```python
        def example_node(state: AgentState, config: RunnableConfig):
            # work happens here ...
            state["status"] = "done"
            return state
        ```
    </Tab>
    <Tab value="TypeScript">
        ```typescript
        function exampleNode(state: AgentState, config: RunnableConfig): AgentState {
            // work happens here ...
            state.status = "done";
            return state;
        }
        ```
    </Tab>
</Tabs>

Once the agent has called this method, and returned the updated state to the client, this change will immediately be reflected in your UI, keeping them in sync.

### Install the CopilotKit SDK
<InstallSDKSnippet components={props.components}/>

### Streaming messages to the chat window

One great use case for state streaming is to present LLM messages to the user, giving them visibility into the agent's progress as it works on tasks. By default, messages from the LLM will be streamed to the user with no additional configuration needed.

<Tabs items={['Python', 'TypeScript']} default="Python">
    <Tab value="Python">
        ```python
        from copilotkit.langgraph import copilotkit_customize_config

        async def talk_to_user_node(state: AgentState, config: RunnableConfig):
            """
            Messages generated in this node will appear in the chat window
            """

            # Pass the modified configuration to LangChain as an argument
            response = await ChatOpenAI(model="gpt-4o").ainvoke(
                [
                    *state["messages"],
                    SystemMessage(content="Say hi to the user")
                ],
                config
            )

            # Then return the new messages, making them part of the
            # conversation history in LangGraph and your chat window
            return {
                "messages": [response]
            }
        ```
    </Tab>
    <Tab value="TypeScript">
        ```typescript
        import { copilotkitCustomizeConfig } from '@copilotkit/sdk-js/langgraph';

        async function talkToUserNode(state: AgentState, config: RunnableConfig): Promise<AgentState> {
            /**
             * Messages generated in this node will appear in the chat window
             */

            // Pass the modified configuration to LangChain as an argument
            const response = await ChatOpenAI({ model: 'gpt-4o' }).invoke(
                [
                    SystemMessage("Say hi to the user"),
                    ...state.messages
                ],
                config
            );

            // Then return the new messages, making them part of the
            // conversation history in LangGraph and your chat window
            return {
                messages: [response]
            };
        }
        ```
    </Tab>
</Tabs>

Note that while this will make the LLM's messages visible in chat, you will need to interrupt the agent if you want the user to respond. You can do this by navigating to an `__END__` node in the agent's workflow, by using LangGraph interrupts, or [exiting the agent](/coagents/advanced/exit-agent).


### Streaming state from LLM tool calls

Often, you want to update your state directly from LLM tool calls, giving the user direct feedback as the agent proceeds through its workflow.

With `emit_intermediate_state`, you can have CopilotKit set a key in your state should be set from a tool call. In this example, we're asking for the `outline` key to be streamed from the `set_outline` tool. You can optionally specify which argument to use, in this case `outline`:

<Tabs items={['Python', 'TypeScript']} default="Python">
    <Tab value="Python">
        ```python
        from copilotkit.langgraph import copilotkit_customize_config

        async def streaming_state_node(state: AgentState, config: RunnableConfig):
            modifiedConfig = copilotkit_customize_config(
                config,

                # this will stream tool calls *as if* they were state
                emit_intermediate_state=[
                    {
                        "state_key": "outline",
                        "tool": "set_outline",
                        "tool_argument": "outline",
                    }
                ]
            )

            # Pass the modified configuration to LangChain as an argument
            response = await ChatOpenAI(model="gpt-4o").bind_tools([set_outline]).ainvoke(
                [
                    *state["messages"]
                ],
                modifiedConfig
            )

            return {
                "messages": [response],
                "outline": outline # <- you still need to return your final state
            }
        ```
    </Tab>
    <Tab value="TypeScript">
        ```typescript
        import { copilotkitCustomizeConfig } from '@copilotkit/sdk-js/langgraph';

        async function streamingStateNode(state: AgentState, config: RunnableConfig): Promise<AgentState> {
            const modifiedConfig = copilotkitCustomizeConfig(
                config,

                // this will stream tool calls *as if* they were state
                {
                    emitIntermediateState: [{
                        stateKey: "outline",
                        tool: "setOutline",
                        toolArgument: "outline",
                    }]
                }
            );

            // Pass the modified configuration to LangChain as an argument
            const response = await ChatOpenAI({ model: 'gpt-4o' }).bindTools([setOutline]).invoke(
                [...state.messages],
                modifiedConfig
            );

            return {
                ...state,
                outline // <- you still need to return your final state
            };
        }
        ```
    </Tab>
</Tabs>

### Calling frontend actions

In addition to syncing state data between the agent and your frontend, CopilotKit also sends the agent information about all the actions available in your application — both backend and frontend — that you can pass directly to LangChain as tools.

This information is provided via the `CopilotKitState` type on the Python side, accessible via the `state.copilotkit` property in your agent code.

To enable calling frontend actions, bind the available actions to the model using `.bind(state.copilotkit.actions`)


Here's a complete example:

<Tabs items={['Python', 'TypeScript']} default="Python">
    <Tab value="Python">
        ```python

        from copilotkit import CopilotKitState

        class AgentState(CopilotKitState):
            # add any additional properties here
            your_additional_properties: str

        def frontend_actions_node(state: AgentState, config: RunnableConfig):

            # 1) Provide the actions to the LLM
            model = ChatOpenAI(model="gpt-4o").bind(state.copilotkit.actions)

            # 2) Call the model
            response = await model.ainvoke(*state, config)

            return {
                "messages": [response]
            }
        ```
    </Tab>
    <Tab value="TypeScript">
        ```typescript

        import { Annotation } from "@langchain/langgraph";
        import { CopilotKitStateAnnotation } from "@copilotkit/sdk-js/langgraph";

        export const AgentStateAnnotation = Annotation.Root({
            // add any additional properties here
            yourAdditionalProperty: Annotation<string>,
            ...CopilotKitStateAnnotation.spec,
        });

        export type AgentState = typeof AgentStateAnnotation.State;

        async function frontendActionsNode(state: AgentState, config: RunnableConfig): Promise<AgentState> {

            // 1) Provide the actions to the LLM
            const model = ChatOpenAI({ model: 'gpt-4o' }).bind(state.copilotkit.actions);

            // 2) Call the model
            const response = await model.invoke(...state, config);

            return {
                messages: [response]
            };
        }
        ```
    </Tab>
</Tabs>

Note that you'll also need to interrupt the execution of the LangGraph agent so that the tool can be executed and the agent can continue with the result from the action.

### Disabling state streaming

Occasionally, you'll want to disable streaming temporarily — for example, the LLM may be doing something the current user should not see, like emitting tool calls or questions pertaining to other employees in an HR system.

You can pass `emit_messages=False` in these cases, which will suppress any messages from being sent to the client from this LLM call.

<Tabs items={['Python', 'TypeScript']} default="Python">
    <Tab value="Python">
        ```python
        from copilotkit.langgraph import copilotkit_customize_config

        def frontend_actions_node(state: AgentState, config: RunnableConfig):
            
            # 1) Configure CopilotKit not to emit messages
            modifiedConfig = copilotkit_customize_config(
                config,
                emit_messages=False,
            )

            # 2) Provide the actions to the LLM
            model = ChatOpenAI(model="gpt-4o").bind(state.copilotkit.actions)

            # 3) Call the model with CopilotKit's modified config
            response = await model.ainvoke(*state, modifiedConfig)

            # don't return the new response to hide it from the user
            return state
        ```
    </Tab>
    <Tab value="TypeScript">
        ```typescript
        import { copilotkitCustomizeConfig } from '@copilotkit/sdk-js/langgraph';

        async function frontendActionsNode(state: AgentState, config: RunnableConfig): Promise<AgentState> {
            // 1) Configure CopilotKit not to emit messages
            const modifiedConfig = copilotkitCustomizeConfig(
                config,
                { emitMessages: false }
            );

            // 2) Provide the actions to the LLM
            const model = ChatOpenAI({ model: 'gpt-4o' }).bind(state.copilotkit.actions);

            // 3) Call the model with CopilotKit's modified config
            const response = await model.invoke(...state, modifiedConfig);

            // don't return the new response to hide it from the user
            return state
        }
        ```
    </Tab>
</Tabs>

The current state that the user is allowed to see will be returned at the end of this method call, and subsequent LLM calls can re-enable streaming with `emit_messages=True`.


<CoAgentsEnterpriseCTA />
