---
title: "Managing state streaming"
---
import { CoAgentsEnterpriseCTA } from "@/components/react/coagents/coagents-enterprise-cta.tsx";

By integrating with LangGraph agents on both the backend and frontend, CopilotKit is able to keep your Agentic Copilot's state in sync with your UI in real time, making it as easy to work with as local state while providing immediate updates as the agent works on requests.

To illustrate, imagine a LangGraph agent starts with an initial state of `{"status": "pending"}`, which would propagate to your CopilotKit frontend as the agent performs tasks along its workflow graph.

At some point, a workflow node changes the state to `{"status": "done"}` after performing its task:

```python
def example_node(state: AgentState, config: RunnableConfig):
	# work happens here ...
	state["status"] = "done"
	return state
```

Once the agent has called this method, and returned the updated state to the client, this change will immediately be reflected in your UI, keeping them in sync.


### Streaming messages to the chat window

One great use case for state streaming is to present LLM messages to the user, giving them visibility into the agent's progress as it works on tasks. By default, messages from the LLM will be streamed to the user with no additional configuration needed.

```python
from copilotkit.langchain import copilotkit_customize_config

async def talk_to_user_node(state: AgentState, config: RunnableConfig):
	"""
	Messages generated in this node will appear in the chat window
	"""

	# Pass the modified configuration to LangChain as an argument
	response = await ChatOpenAI(model="gpt-4o").ainvoke(
		[
			*state["messages"],
			SystemMessage(content="Say hi to the user")
		],
		config
	)

	# Then return the new messages, making them part of the
	# conversation history in LangGraph and your chat window
	return {
		"messages": [response]
	}
```

Note that while this will make the LLM's messages visible in chat, you will need to interrupt the agent if you want the user to respond. You can do this by navigating to an `__END__` node in the agent's workflow, by using LangGraph interrupts, or [exiting the agent](/coagents/advanced/exit-agent).


### Streaming state from LLM tool calls

Often, you want to update your state directly from LLM tool calls, giving the user direct feedback as the agent proceeds through its workflow.

With `emit_intermediate_state`, you can have CopilotKit set a key in your state should be set from a tool call. In this example, we're asking for the `outline` key to be streamed from the `set_outline` tool. You can optionally specify which argument to use, in this case `outline`:

```python
from copilotkit.langchain import copilotkit_customize_config

async def streaming_state_node(state: AgentState, config: RunnableConfig):
	modifiedConfig = copilotkit_customize_config(
	    config,

	    # this will stream tool calls *as if* they were state
	    emit_intermediate_state=[
		    {
			    "state_key": "outline",
			    "tool": "set_outline",
			    "tool_argument": "outline",
		    }
	    ]
	)

	# Pass the modified configuration to LangChain as an argument
	response = await ChatOpenAI(model="gpt-4o").bind_tools([set_outline]).ainvoke(
		[
			*state["messages"]
		],
		modifiedConfig
	)

	return {
		*state,
		"outline": outline # <- you still need to return your final state
	}
```

### Calling frontend actions

In addition to syncing state data between the agent and your frontend, CopilotKit also sends the agent information about all the actions available in your application — both backend and frontend — that you can pass directly to LangChain as tools.

This information is provided via the `CopilotKitState` type on the Python side, accessible via the `state.copilotkit` property in your agent code.

To enable calling frontend actions: 

1. Set `emit_tool_calls=True` in `copilotkit_customize_config`
2. Bind the available actions to the model using `.bind(state.copilotkit.actions`)
3. Pass in the modified config when calling the model

Here's a complete example:

```python
from copilotkit.langchain import copilotkit_customize_config

def frontend_actions_node(state: AgentState, config: RunnableConfig):
	
	# 1) Configure CopilotKit to emit tool calls
	modifiedConfig = copilotkit_customize_config(
	    config,
	    emit_tool_calls=True,
	)

  # 2) Provide the actions to the LLM
	model = ChatOpenAI(model="gpt-4o").bind(state.copilotkit.actions)

	# 3) Call the model with CopilotKit's modified config
	response = await model.ainvoke(*state, modifiedConfig)

	return {
    "messages": [response]
  }
```

Note that you'll also need to interrupt the execution of the LangGraph agent so that the tool can be executed and the agent can continue with the result from the action.

### Disabling state streaming

Occasionally, you'll want to disable streaming temporarily — for example, the LLM may be doing something the current user should not see, like emitting tool calls or questions pertaining to other employees in an HR system.

You can pass `emit_messages=False` in these cases, which will suppress any messages from being sent to the client from this LLM call.

```python
from copilotkit.langchain import copilotkit_customize_config

def frontend_actions_node(state: AgentState, config: RunnableConfig):
	
	# 1) Configure CopilotKit to emit tool calls
	modifiedConfig = copilotkit_customize_config(
	    config,
	    emit_messages=False,
	)

  # 2) Provide the actions to the LLM
	model = ChatOpenAI(model="gpt-4o").bind(state.copilotkit.actions)

	# 3) Call the model with CopilotKit's modified config
	response = await model.ainvoke(*state, modifiedConfig)

	return {
    "messages": [response]
  }
```

The current state that the user is allowed to see will be returned at the end of this method call, and subsequent LLM calls can re-enable streaming with `emit_messages=True`.


<CoAgentsEnterpriseCTA />
