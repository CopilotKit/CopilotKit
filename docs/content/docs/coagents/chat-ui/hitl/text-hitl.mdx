---
title: Agent Q&A via text chat
---

import { CoAgentsEnterpriseCTA } from "@/components/react/coagents/coagents-enterprise-cta.tsx";
import { Accordion, Accordions } from 'fumadocs-ui/components/accordion';

The simplest way for an AI agent to include humans in the loop is to respond to a question with a question. While CopilotKit and CoAgents enable far more sophisticated workflows, here's a simple example to illustrate how agents and humans can converse.

These examples are drawn from the `coagents-qa-text` sample code; you can [get it from our GitHub repo](https://github.com/copilotkit/copilotkit/tree/main/examples/coagents-qa-text) and follow along or run it for yourself.

## In the React UI

Because we're relying only on chat-based UI, the React code for this interaction is very simple, consisting only of the `CopilotPopup` pre-built component and some instruction text:

```tsx
"use client";

import { CopilotPopup } from "@copilotkit/react-ui";

export function Greeter() {
  return (
    <div className="flex flex-col items-center justify-center h-screen">
      <div className="text-2xl">Text Q&A example</div>
      <div>ask: "Greet me!"</div>

      <CopilotPopup defaultOpen={true} clickOutsideToClose={false} />
    </div>
  );
}
```

## In the Python agent

<Accordions>

<Accordion title="Complete Python agent code">

```python
"""Test Q&A Agent"""

import os
from typing import cast
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langgraph.graph import StateGraph, END
from langgraph.graph import MessagesState
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.runnables import RunnableConfig
from langchain_core.messages import HumanMessage, AIMessage
from copilotkit.langchain import (
    copilotkit_exit,
    copilotkit_emit_message
)
from pydantic import BaseModel, Field

def get_model():
    """
    Get a model based on the environment variable.
    """
    model = os.getenv("MODEL", "openai")

    if model == "openai":
        return ChatOpenAI(temperature=0, model="gpt-4o")
    if model == "anthropic":
        return ChatAnthropic(
            temperature=0,
            model_name="claude-3-5-sonnet-20240620",
            timeout=None,
            stop=None
        )

    raise ValueError("Invalid model specified")


class GreetAgentState(MessagesState):
    """Greet Agent State"""
    name: str

class ExtractNameTool(BaseModel):
    """
    Extract the user's name from the message.
    Make sure to only set the name if you are 100 percent sure it is the name of the user.
    """
    name: str = Field(..., description="The user's name or UNKNOWN if you can't find it")

async def ask_name_node(state: GreetAgentState, config: RunnableConfig):
    """
    Ask the user for their name.
    """

    await copilotkit_emit_message(config, "Hey, what is your name? ðŸ™‚")

    return {
        "messages": state["messages"],
    }


async def extract_name_node(state: GreetAgentState, config: RunnableConfig):
    """
    Check if the user's name is in the message.
    """

    last_message = cast(HumanMessage, state["messages"][-1])


    instructions = (
        f"Figure out the user's name if possible from this response they gave you: {last_message.content}"
    )

    model = get_model().bind_tools(
        [ExtractNameTool],
        tool_choice="ExtractNameTool"
    )

    response = await model.ainvoke([
        *state["messages"],
        HumanMessage(
            content=instructions
        )
    ], config)

    tool_calls = cast(AIMessage, response).tool_calls
    name = None

    if tool_calls is not None:
        if tool_calls[0]["args"]["name"] != "UNKNOWN":
            name = tool_calls[0]["args"]["name"]

    if name is None:
        return {
            "messages": state["messages"],
        }

    return {
        "messages": state["messages"],
        "name": name,
    }

async def greet_node(state: GreetAgentState, config: RunnableConfig):
    """
    Greet the user by name.
    """

    await copilotkit_emit_message(config, "Hello, " + state["name"] + " ðŸ˜Ž")

    await copilotkit_exit(config)

    return {
        "messages": state["messages"],
    }

def route(state: GreetAgentState):
    """Route to the appropriate node."""

    if state.get("name", None) is not None:
        return "greet_node"
    return "ask_name_node"

workflow = StateGraph(GreetAgentState)

workflow.add_node("ask_name_node", ask_name_node)
workflow.add_node("greet_node", greet_node)
workflow.add_node("extract_name_node", extract_name_node)

workflow.set_entry_point("ask_name_node")

workflow.add_edge("ask_name_node", "extract_name_node")
workflow.add_conditional_edges("extract_name_node", route)
workflow.add_edge("greet_node", END)
memory = MemorySaver()
graph = workflow.compile(checkpointer=memory, interrupt_after=["ask_name_node"])
```
</Accordion>
</Accordions>

While this agent code can seem verbose, the workflow is simple:

- The user will say "hello", "greet me" or something similar to initiate a conversation
- The agent will ask the user their name
- The user will respond, and if their response contains a recognizable name the LLM will save that to shared state
- The agent will greet the user by name

To make sure the agent asks the user their name, the graph's entry point is set to the `ask_name_node` method, which includes an explicit call to `copilotkit_emit_message`:

```python
async def ask_name_node(state: GreetAgentState, config: RunnableConfig):
    """
    Ask the user for their name.
    """

    await copilotkit_emit_message(config, "Hey, what is your name? ðŸ™‚")

    return {
        "messages": state["messages"],
    }
```

`copilotkit_emit_message` imperatively sends a message to the user as if it were generated by the LLM, directing the flow of conversation with the chatbot.

The user's response will be handled by the next node in the graph â€” `extract_name_node` â€” which will ask the LLM to (as it says) extract the user's name from whatever they sent, passing it to the `ExtractNameTool` model class whose `name` property is included in the agent's state.

```python
class ExtractNameTool(BaseModel):
    """
    Extract the user's name from the message.
    Make sure to only set the name if you are 100 percent sure it is the name of the user.
    """
    name: str = Field(..., description="The user's name or UNKNOWN if you can't find it")

async def extract_name_node(state: GreetAgentState, config: RunnableConfig):
    """Check if the user's name is in the message."""

    last_message = cast(HumanMessage, state["messages"][-1])

    # This instructions prompt tells the LLM to extract the name
    instructions = (
        f"Figure out the user's name if possible from this response they gave you: {last_message.content}"
    )

    model = get_model().bind_tools(
        [ExtractNameTool],
        tool_choice="ExtractNameTool"
    )

    response = await model.ainvoke([
        *state["messages"],
        HumanMessage(
            content=instructions
        )
    ], config)

    return {
        "messages": state["messages"],
        "name": name,
    }
```

Here's the finished Q&A in action:

<video src="/images/coagents/copilotkit-qa-text-example-hires.mp4" controls className="w-full [aspect-ratio:90/72] max-w-xl mx-auto" />


<CoAgentsEnterpriseCTA />