---
title: "Quickstart"
description: "Get started with Agentic Copilots in just a few minutes."
icon: "lucide/Play"
---

import { TailoredContent, TailoredContentOption } from "@/components/react/tailored-content.tsx";
import { CoAgentsEnterpriseCTA } from "@/components/react/coagents/coagents-enterprise-cta.tsx";
import { CoAgentsDiagram } from "@/components/react/coagents/coagents-diagram.tsx";
import { FaPython, FaJs, FaCloud } from "react-icons/fa";
import SelfHostingCopilotRuntimeCreateEndpoint from "@/snippets/self-hosting-copilot-runtime-create-endpoint.mdx";
import { InsecurePasswordProtected } from "@/components/react/insecure-password-protected.tsx";

## Before you start

Agentic copilots integrate with LangGraph-powered agents which are either deployed to the LangGraph Platform or self-hosted with a FastAPI endpoing.

The LangGraph Platform deployment is recommended and supports both LangGraph.js and LangGraph python.
Raw self-hosting is currently only offered via CopilotKit's FastAPI-powered remote-endpoint.


**This guide assumes youâ€™re familiar with using LangGraph to build agent workflows.** If you need a quick introduction, 
check out [this brief example from the LangGraph docs](https://langchain-ai.github.io/langgraph/).

## Getting started

In order for CopilotKit to integrate with your LangGraph agent, it has to be deployed either by yourself or on LangGraph Platform.


<TailoredContent>
<TailoredContentOption 
    title="Remote Endpoint (FastAPI / Python)"
    description="I want to write my LangGraph agent in Python and self-host it with a FastAPI endpoint."
    icon={<FaPython />}
  >
    **For orientation, here's how agentic copilots are wired:**
    <CoAgentsDiagram />

    <Steps>
      <Step>
        First, take a minute to **[go through the CopilotKit quickstart](/quickstart), and integrate CopilotKit into your React app.**
        This should only take a minute.

        <Callout type="info">
          Note! that if you are planning to use only a single LangGraph agent as your app's agentic backend,
          you can use Agent-Lock mode (see below), and skip setting up an LLM adapter.
        </Callout>
      </Step>

      <Step>
        Next, follow the [Remote Endpoint quickstart](/guides/backend-actions/remote-backend-endpoint) to setup a FastAPI Remote-Endpoint.<br/>
        **This endpoint will serve your (Python) LangGraph agent.**
      </Step>

      <Step>
        ## Connect your FastAPI Remote Endpoint to a LangGraph agent

        At this point you should have CopilotKit hooked into your application, connected to a FastAPI Remote Endpoint.

        The next step is to configure the Remote Endpoint to serve LangGraph agents.

        First, find your `CopilotKitSDK` instance in your Python Remote Endpoint (typically `server.py`). <br/>
        **Then modify your `CopilotKitSDK` instance (setup in the previous step) to serve LangGraph agents:**

        ```python title="server.py"
        from copilotkit import CopilotKitSDK, LangGraphAgent # [!code highlight]
        from .agent import the_langraph_graph # Import your LangGraph agent; in this example, it's the variable named `the_langraph_graph` in ./agent.py # [!code highlight]
        from copilotkit.integrations.fastapi import add_fastapi_endpoint
        from copilotkit.langchain import copilotkit_messages_to_langchain # you only need this if you use Google Gemini in your LangGraph agent.
        # ... other imports

        app = FastAPI()

        # ...

        # Initialize the CopilotKit SDK
        sdk = CopilotKitSDK(
          agents=[ # [!code highlight:10]
            LangGraphAgent(
              name="basic_agent",
              description="Agent that answers questions about the weather",
              graph=the_langraph_graph,
              # copilotkit_config={ # if you use Google Gemini, uncomment  this code (and import `copilotkit_messages_to_langchain`, see above)
              #  "convert_messages": copilotkit_messages_to_langchain(use_function_call=True)
              # }
            )
          ],
          # ...
        )

        # ...

        # Add the CopilotKit endpoint to your FastAPI app
        add_fastapi_endpoint(app, sdk, "/copilotkit_remote")

        # ...

        ```

        <Callout>
          Remember the name `basic_agent`, we'll need it as we move on to integrating
          this agent into the frontend.
        </Callout>

        ### Are you using Google Gemini models?

        Until there is full parity between Gemini and other models in the LangChain ecosystem, you'll need to uncomment the `copilotkit_config` section shown above when using Gemini models in your LangGraph agent.
      </Step>

      <Step>
        ## Agent-lock your Copilot to the `basic_agent` agent.

        CopilotKit supports router-mode as well as agent-lock mode. For more detail see [router-mode / agent-mode](/coagents/advanced/router-mode-agent-lock).
        In short: agent-lock modes locks your Copilot to a single LangGraph agent;
        router-mode automatically routes requests to the right agent based on the user's context and query.

        **For simplicity, we'll use agent-lock mode in these tutorials,** but we encourage you to explore router-mode in your production use-cases to support more complex agent workflows.

        Lock the Copilot to the `basic_agent` setup earlier. This means every single interaction with the Copilot will be forwarded to the locked agent.

        ```tsx filename="src/page.tsx"
        // The Copilot will now invoke the LangGraph agent directly, not the Copilot router.
        <CopilotKit
          runtimeUrl="/api/copilotkit"
          agent="basic_agent" // agent-lock the Copilot, see 'agent-lock vs router-mode' // [!code highlight]
        >
          {...}
        </CopilotKit>
        ```
      </Step>
    </Steps>
  </TailoredContentOption>
  <TailoredContentOption
    title="LangGraph Platform (recommended)"
    description="I want to write my LangGraph agent in Python or JavaScript and deploy it through LangGraph Platform."
    icon={<FaCloud />}
  >
    <InsecurePasswordProtected>
      <Steps>
        <Step>
          First, take a minute to **[go through the CopilotKit quickstart](/quickstart), and integrate CopilotKit into your React app.**
          This should only take a minute.

          <Callout type="info">
            Note! that if you are planning to use only a single LangGraph agent as your app's agentic backend,
            you can use Agent-Lock mode (see below), and skip setting up an LLM adapter.
          </Callout>

        </Step>

        <Step>
          ## Deploy your LangGraph agent

          <Callout type="info">
            **If you have already deployed your LangGraph agent, skip to step 4.**
          </Callout>




          <Tabs items={['LangGraph Studio (local development)', 'LangGraph Platform (cloud)']}>

            <Tab value="LangGraph Studio (local development)">
              For local deployment during development, you can [use LangGraph Studio](https://langchain-ai.github.io/langgraph/cloud/how-tos/test_local_deployment/).
              <Callout type="info">
                To ensure a successful deployment, follow these steps from the LangGraph Studio documentation:
                1. Make sure your agent folder contains a `langgraph.json` file.
                2. In the `langgraph.json` file, reference the path to a `.env` file.
                3. In the `.env` file, include your `LANGSMITH_API_KEY`.
              </Callout>

              A successful deployment will yield an API URL (often referred to here as "deployment URL"). 
              It will generally look like this: `http://localhost:63899`.

              Come back with that URL and a LangSmith API key before proceeding.

              <img src="/images/langgraph-studio-local-url.png" alt="LangGraph Studio Local URL" className="my-4" />
            </Tab>

            <Tab value="LangGraph Platform (cloud)">
              For production, you can deploy to LangGraph Platform by following the official [LangGraph Platform deployment guide](https://langchain-ai.github.io/langgraph/cloud/deployment/cloud/).

              A successful deployment will yield an API URL (often referred to here as "deployment URL"). 
              It will look like this: `https://{project-identifiers}.langgraph.app`.

              Come back with that URL and a LangSmith API key before proceeding.
            </Tab>

          </Tabs>
        </Step>


        <Step>
          ## Find your CopilotRuntime instance

          Now that you've deployed your agent to LangGraph Platform, you will need to integrate it into your CopilotKit application.

          First, find your CopilotRuntime instance in your code. If you followed the quickstart, it'll be where you set up the
          `/api/copilotkit` endpoint.
        </Step>

        <Step>
          ## Connect the app to the remote endpoint

          Once you've found your runtime instance, you can connect your app to the remote endpoint by modifying your CopilotRuntime configuration.

          ```tsx
            const runtime = new CopilotRuntime({
              // ...existing configuration
              remoteEndpoints: [ 
                langGraphPlatformEndpoint({
                  deploymentUrl: "your-api-url", // make sure to replace with your real deployment url // [!code highlight]
                  langsmithApiKey: process.env.LANGSMITH_API_KEY, // only used in LangGraph Cloud deoployments
                  agents: [ // List any agents available under "graphs" list in your langgraph.json file; give each a description explaining when it should be called.
                    {
                      name: 'my_agent', // this name must match one of your graphs defined under the `langgraph.json` file  // [!code highlight]
                      description: 'A helpful LLM agent that should be used whenever the user needs assistance with general queries or tasks.' // [!code highlight]
                    }
                  ]
                }),
              ],
            });
          ```

          <Callout>
            Remember to replace `your-api-url` with your actual LangGraph Platform deployment URL and
            set your LangSmith API key as an environment variable.
          </Callout>
        </Step>

        <Step>
          ## (optional) Agent-lock your Copilot to the `basic_agent` agent.

          CopilotKit supports router-mode as well as agent-lock mode. For more detail see our concept documentation on [multi-agent flows](/coagents/concepts/multi-agent-flows).

          In short:
          - Agent-lock mode locks your Copilot to a single LangGraph agent;
          - Router mode automatically routes requests to the right agent based on the user's context and query.

          **For simplicity, we'll use agent-lock mode in these tutorials,** but we encourage you to explore router-mode in your production use-cases to support more complex agent workflows.

          Lock the Copilot to the `my_agent` which you setup in the previous step. Doing this will ensure every single interaction with the Copilot will be forwarded to the locked agent.

          ```tsx filename="src/page.tsx"
            // The Copilot will now invoke the LangGraph agent directly, not the Copilot router.
            <CopilotKit
              runtimeUrl="/api/copilotkit"
              agent="my_agent" // agent-lock the Copilot, see 'agent-lock vs router-mode' // [!code highlight]
            >
              {...}
            </CopilotKit>
          ```
        </Step>
      </Steps>
    </InsecurePasswordProtected>
  </TailoredContentOption>
</TailoredContent>

ðŸŽ‰ Congrats! You've successfully integrated a LangGraph agent chatbot to your application!
Give it a try by writing text in the chatbot UI and pressing send.
You are now chatting with a LangGraph agent!

<Frame className="my-6">
  <img
    src="/images/coagents/CoAgents-ChatHello.gif"
    alt="Agentic copilots Chat Hello demonstration"
    className="w-2/3 mx-auto"
  />
</Frame>

<CoAgentsEnterpriseCTA />
